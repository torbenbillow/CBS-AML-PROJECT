{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/torbenbillow/CBS-AML-PROJECT/blob/main/notebooks/00_welcome.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORT LIBRARIES"
      ],
      "metadata": {
        "id": "FSKTe-rY5QJL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "id": "GEVrLPEfeP5M"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point, Polygon\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n",
        "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
        "from sklearn.svm import SVR, SVC\n",
        "\n",
        "from sklearn.metrics import (mean_absolute_error, mean_squared_error, r2_score,\n",
        "                             accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, confusion_matrix, classification_report,\n",
        "                             roc_auc_score, roc_curve, max_error, mean_absolute_percentage_error)\n",
        "\n",
        "from google.colab import drive\n",
        "pd.options.mode.copy_on_write = True"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOAD DATA FROM GOOGLE DRIVE"
      ],
      "metadata": {
        "id": "ok1UbZMG5Bwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading raw data from Gdrive location...\")\n",
        "file_id = \"1Iyr7zX8u0gKWKUWCSgZUpcvbTuYBmw1V\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "listings_raw = pd.read_csv(url)\n",
        "print(\"Raw Data Shape:\",listings_raw.shape)\n",
        "\n",
        "print(\"Loading feature selection data from Gdrive location...\")\n",
        "file_id = \"1TcMwa1S699swnO-CSq3vpzl6fdE0Xzln\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "feature_selection = pd.read_excel(url)\n",
        "print(\"Feature Selection Shape:\",feature_selection.shape)"
      ],
      "metadata": {
        "id": "ML5gnboD96C5",
        "outputId": "1f1b604c-85a0-442b-e4d0-bf69420b9d58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading raw data from Gdrive location...\n",
            "Raw Data Shape: (22684, 79)\n",
            "Loading feature selection data from Gdrive location...\n",
            "Feature Selection Shape: (90, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA WRANGLING"
      ],
      "metadata": {
        "id": "3f4Y7ESlezbo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ADD A CODE SNIPPET HERE TO PRINT AN OVERVIEW OF THE DATA WITH POTENTIAL ISSUES THAT WE FIX BELOW"
      ],
      "metadata": {
        "id": "cNv_KmHRF5WV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FILTER THE DATASET TO REMOVE LISTINGS WITHOUT REVIEWS AND WITH NO OCCUPANCY IN THE LAST YEAR"
      ],
      "metadata": {
        "id": "4zN4rqJMcYpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data as df.\n",
        "df = listings_raw.copy()\n",
        "\n",
        "''' Remove listings without reviews in 2025 and no occupancy in the last year. We have chosen to filter \"last_review\" for listings that have\n",
        "a review within 6 months of the scrape date and \"estimated_occupancy_l365d\" for listings that have a non-zero value. We have done this to ensure\n",
        "we are including only listings that are active, and thereby more informative for our model. '''\n",
        "df = df.query(\n",
        "    \"last_review >= '2025-01-01' and estimated_occupancy_l365d > 0\"\n",
        ")"
      ],
      "metadata": {
        "id": "7oyzHoag71Fw"
      },
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check sizes of data and filtered data\n",
        "print(\"Before filtering:\",listings_raw.shape, \"\\nAfter filtering:\",df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nzRHrvQavYQ",
        "outputId": "86e53ee6-b48a-4c63-b6c4-a356e447eb92"
      },
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before filtering: (22684, 79) \n",
            "After filtering: (10132, 79)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MANUALLY DROP UNWANTED COLUMNS (DOMAIN KNOWLEDGE)"
      ],
      "metadata": {
        "id": "SPIFFtJe_pYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check size of data frame before drops\n",
        "print(\"Before column drops:\",df.shape)\n",
        "\n",
        "# The following columns are irrelevant, and thus, we have chosen to drop them.\n",
        "df = df.drop(columns=[\"id\",\"listing_url\",\"scrape_id\",\"source\",\"host_id\",\"host_url\",\"calendar_last_scraped\",\"first_review\",\"host_neighbourhood\"])\n",
        "\n",
        "# The following columns could impact a listing's \"success\" (discrimination, etc.), but due to the complexity and the scope of this project,\n",
        "# we have chosen to drop it.\n",
        "df = df.drop(columns=[\"name\",\"picture_url\",\"host_name\",\"host_thumbnail_url\",\"host_picture_url\"])\n",
        "\n",
        "# Dropping the following columns in favor of \"calculated_host_listings_count,\" because that is the direct calculation of how many listings\n",
        "# a host has at the time of scrape. The metadata also notes that this calculation is \"unknown,\" and thereby less trustworthy.\n",
        "df = df.drop(columns=[\"host_listings_count\",\"host_total_listings_count\"])\n",
        "\n",
        "# Dropping in favor of \"host_identitity_verified,\" which is a boolean that indicates whether or not the host is verified.\n",
        "df = df.drop(columns=[\"host_verifications\"])\n",
        "\n",
        "# Dropping in favor of \"neighborhood_cleansed.\"\n",
        "df = df.drop(columns=[\"neighbourhood\"])\n",
        "\n",
        "# Dropping the following, as they are completely empty attributes.\n",
        "df = df.drop(columns=[\"neighbourhood_group_cleansed\",\"calendar_updated\",\"license\"])\n",
        "\n",
        "# Dropping the following columns in favor of \"mini_nights\" and \"maximum_nights.\"\n",
        "df = df.drop(columns=[\"minimum_minimum_nights\",\"maximum_minimum_nights\",\"minimum_maximum_nights\",\"maximum_maximum_nights\",\"minimum_nights_avg_ntm\",\"maximum_nights_avg_ntm\"])\n",
        "\n",
        "# Dropping the following columns, because there is no information about they represent.\n",
        "df = df.drop(columns=[\"has_availability\",\"availability_eoy\"])\n",
        "\n",
        "''' Dropping the following columns, because we have selected \"availability_30\" as our target variable. We believe this the most accurate\n",
        "indicator of a listing's popularity/success, because listings will on average be booked more in the short term than in the long term.\n",
        "We would be able to attribute the availability to an actual \"interest level,\" rather than a lack of booking simply due to time considerations.\n",
        "Seasonality should not matter since the scope of our listings is limited to Copenhagen, so theoretically, all listings would be impacted equally\n",
        "by seasonality. '''\n",
        "df = df.drop(columns=[\"availability_60\",\"availability_90\",\"availability_365\"])\n",
        "\n",
        "# Dropping the following columns in favor \"number_of_reviews\" for simplicity.\n",
        "df = df.drop(columns=[\"number_of_reviews_ltm\",\"number_of_reviews_l30d\"])\n",
        "\n",
        "# Dropping in favor \"number_of_reviews\" for simplicity.\n",
        "df = df.drop(columns=[\"number_of_reviews_ly\"])\n",
        "\n",
        "# We have chosen to filter this column for listings that have a nonzero value. We have done this to ensure we are only including listings\n",
        "# that are active, and thereby more informative for our model. MAKE A NOTE OF SELECTION BIAS.\n",
        "df = df.drop(columns=[\"estimated_occupancy_l365d\"])\n",
        "\n",
        "# Irrelevant and would introduce data leakage.\n",
        "df = df.drop(columns=[\"estimated_revenue_l365d\"])\n",
        "\n",
        "# We have chosen to filter this column for listings that have a review within 6 months of the scrape date. We have done this to ensure we\n",
        "# are only including listings that are active, and thereby more informative for our model. MAKE A NOTE OF SELECTION BIAS.\n",
        "df = df.drop(columns=[\"last_review\"])\n",
        "\n",
        "# Dropping the following attributes because they will not be available at the time of prediction.\n",
        "df = df.drop(columns=[\"review_scores_rating\",\"review_scores_accuracy\",\"review_scores_cleanliness\",\"review_scores_checkin\",\"review_scores_communication\",\"review_scores_location\",\"review_scores_value\"])\n",
        "\n",
        "# Dropping the following columns in favor of \"calculated_host_listings_count\" for simplicity.\n",
        "df = df.drop(columns=[\"calculated_host_listings_count_entire_homes\",\"calculated_host_listings_count_private_rooms\",\"calculated_host_listings_count_shared_rooms\"])\n",
        "\n",
        "# Dropping in favor of \"number_of_reviews.\"\n",
        "df = df.drop(columns=[\"reviews_per_month\"])\n",
        "\n",
        "# Check size of data frame after drops\n",
        "print(\"After column drops:\",df.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVKvv395_oZj",
        "outputId": "c024ecfd-fa9b-4e42-8ef6-57a3af63d2b0"
      },
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before column drops: (10132, 79)\n",
            "After column drops: (10132, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLEAN UP \"price\" ATTRIBUTE\n",
        "\n"
      ],
      "metadata": {
        "id": "vcwLLq8kS9qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove $, commas, and spaces, then convert to numeric\n",
        "price_clean = (\n",
        "    df[\"price\"]\n",
        "    .astype(str)                             # handle existing ints / NA\n",
        "    .str.replace(r'[\\$,]', '', regex=True)   # remove $ and commas\n",
        "    .str.strip()\n",
        ")\n",
        "\n",
        "# Convert to numeric, coercing bad values (like '<NA>') to NaN\n",
        "price_numeric = pd.to_numeric(price_clean, errors=\"coerce\")\n",
        "\n",
        "# Store back as nullable integer\n",
        "df[\"price\"] = price_numeric.astype(\"Int64\")"
      ],
      "metadata": {
        "id": "OKGnV3WfS_HV"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VALIDATE PRICE ATTRIBUTE"
      ],
      "metadata": {
        "id": "Iyi71gAMMQIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"price\"].describe()"
      ],
      "metadata": {
        "id": "WX04EAYhMUnm",
        "outputId": "5183e013-652b-43f7-d8f2-eaa6e05967ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count        7872.0\n",
              "mean     1374.50216\n",
              "std      1167.65617\n",
              "min           202.0\n",
              "25%           881.0\n",
              "50%          1168.0\n",
              "75%          1600.0\n",
              "max         63418.0\n",
              "Name: price, dtype: Float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>7872.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1374.50216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1167.65617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>202.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>881.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1168.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>63418.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> Float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 234
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CREATE TWO DATAFRAMES WITH DIFFERENT \"price\" APPROACHES"
      ],
      "metadata": {
        "id": "0YnmQHfpXdT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create two new dataframes to split between approaches (dropping missing prices vs. imputing missing prices)\n",
        "df_price_imputation = df.copy()\n",
        "df_drop_price = df.copy()\n",
        "\n",
        "median_price = df[\"price\"].median()\n",
        "df_price_imputation[\"price\"] = df_price_imputation[\"price\"].fillna(median_price).astype(\"Int64\")\n",
        "df_drop_price = df_drop_price.dropna(subset=[\"price\"])\n",
        "\n",
        "print(\"Shape of df with price imputation:\", df_price_imputation.shape)\n",
        "print(\"Shape of df with dropped blank prices:\", df_drop_price.shape)"
      ],
      "metadata": {
        "id": "hL6prWx2-z8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7de95c70-6d92-47bd-c9ce-54c37c4ac7d1"
      },
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of df with price imputation: (10132, 30)\n",
            "Shape of df with dropped blank prices: (7872, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary with the two split datasets.\n",
        "# Instead of rewriting the pipeline for each version, we can loop over the dataset dictionary and train/evaluate all models on all variants cleanly.\n",
        "\n",
        "datasets = {\n",
        "    \"drop_missing_prices\": df_drop_price,\n",
        "    \"impute_price\": df_price_imputation\n",
        "}\n",
        "\n",
        "print(type(datasets))\n",
        "for name, df in datasets.items():\n",
        "    print(name, type(df))"
      ],
      "metadata": {
        "id": "54vjBoC3dL1M",
        "outputId": "41396934-b21e-40ea-cc7e-8f695d1c8aab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "drop_missing_prices <class 'pandas.core.frame.DataFrame'>\n",
            "impute_price <class 'pandas.core.frame.DataFrame'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLEAN UP BOOLEAN COLUMNS"
      ],
      "metadata": {
        "id": "Zki9UNYxXw7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some cols have boolean values \"f\" and \"t\", we change to 0 or 1\n",
        "bool_cols = [\n",
        "    \"host_is_superhost\",\n",
        "    \"host_has_profile_pic\",\n",
        "    \"host_identity_verified\",\n",
        "    \"instant_bookable\"\n",
        "]\n",
        "\n",
        "bool_map = {\n",
        "    \"t\": 1,\n",
        "    \"f\": 0,\n",
        "    \"true\": 1,\n",
        "    \"false\": 0,\n",
        "    True: 1,\n",
        "    False: 0,\n",
        "}\n",
        "\n",
        "# Fix boolean values in the datasets\n",
        "for name, df in datasets.items():\n",
        "  for col in bool_cols:\n",
        "      df[col] = (df[col].map(bool_map).astype(\"Int64\"))\n",
        "\n",
        "# Fill missing superhost values with 0 in the datasets\n",
        "for name, df in datasets.items():\n",
        "  df[\"host_is_superhost\"] = df[\"host_is_superhost\"].fillna(0).astype(\"Int64\")\n",
        "\n",
        "# Print results\n",
        "for name, df in datasets.items():\n",
        "  print(f\"Cleaned up column information for {name} dataframe...\\n\")\n",
        "  for col in bool_cols:\n",
        "    print(f\"{col} unique values:\", df[col].unique(),\"\\n\")"
      ],
      "metadata": {
        "id": "aHr-MFgzX3cL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8949f38-0c9c-4886-841c-d8e36d494456"
      },
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned up column information for drop_missing_prices dataframe...\n",
            "\n",
            "host_is_superhost unique values: <IntegerArray>\n",
            "[1, 0]\n",
            "Length: 2, dtype: Int64 \n",
            "\n",
            "host_has_profile_pic unique values: <IntegerArray>\n",
            "[1, <NA>, 0]\n",
            "Length: 3, dtype: Int64 \n",
            "\n",
            "host_identity_verified unique values: <IntegerArray>\n",
            "[1, 0, <NA>]\n",
            "Length: 3, dtype: Int64 \n",
            "\n",
            "instant_bookable unique values: <IntegerArray>\n",
            "[0, 1]\n",
            "Length: 2, dtype: Int64 \n",
            "\n",
            "Cleaned up column information for impute_price dataframe...\n",
            "\n",
            "host_is_superhost unique values: <IntegerArray>\n",
            "[1, 0]\n",
            "Length: 2, dtype: Int64 \n",
            "\n",
            "host_has_profile_pic unique values: <IntegerArray>\n",
            "[1, <NA>, 0]\n",
            "Length: 3, dtype: Int64 \n",
            "\n",
            "host_identity_verified unique values: <IntegerArray>\n",
            "[1, 0, <NA>]\n",
            "Length: 3, dtype: Int64 \n",
            "\n",
            "instant_bookable unique values: <IntegerArray>\n",
            "[0, 1]\n",
            "Length: 2, dtype: Int64 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can see based on the information below, that a number of listings have a missing verification status and profile picture.\n",
        "for name, df in datasets.items():\n",
        "  print(f\"{name} results:\")\n",
        "  print(df[[\"host_has_profile_pic\",\"host_identity_verified\"]].isna().sum(),\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMhHzjpOfVZ7",
        "outputId": "54aa9b51-99ce-412c-e05f-e5a9b0b3f396"
      },
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drop_missing_prices results:\n",
            "host_has_profile_pic      236\n",
            "host_identity_verified    236\n",
            "dtype: int64 \n",
            "\n",
            "impute_price results:\n",
            "host_has_profile_pic      257\n",
            "host_identity_verified    257\n",
            "dtype: int64 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Given the relatively small number of null rows, we have chosen to drop them rather than impute values.\n",
        "\n",
        "subset_cols = [\"host_has_profile_pic\", \"host_identity_verified\"]\n",
        "\n",
        "for name, df in datasets.items():\n",
        "    # Drop rows with NA in any of the subset columns\n",
        "    df.dropna(subset=subset_cols, inplace=True)\n",
        "\n",
        "    # Show remaining NA counts for those columns\n",
        "    print(f\"{name} results:\")\n",
        "    print(df[subset_cols].isna().sum(),\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oF6zZDFJYPyl",
        "outputId": "7c2e24d6-d817-418c-f87a-286c58ccb88a"
      },
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drop_missing_prices results:\n",
            "host_has_profile_pic      0\n",
            "host_identity_verified    0\n",
            "dtype: int64 \n",
            "\n",
            "impute_price results:\n",
            "host_has_profile_pic      0\n",
            "host_identity_verified    0\n",
            "dtype: int64 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLEAN UP MISSING \"bed\" AND \"bedroom\" ATTRIBUTES"
      ],
      "metadata": {
        "id": "p3qS5NsS79xh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' There are three measures for how many people a property fits:\n",
        "1. \"accommodates\" = max number of guests\n",
        "2. \"bedrooms\" = number of bedrooms\n",
        "3. \"beds\" = number of beds '''\n",
        "\n",
        "for name, df in datasets.items():\n",
        "  print(f\"{name} results:\\n\",df[['accommodates','bedrooms','beds']].isna().sum(),\"\\n\")\n",
        "\n",
        "# \"accommodates\" has no missingness, while the others do, so we use combinations of them to impute values\n",
        "# We will use all for the training\n",
        "\n",
        "# THIS RESULT COULD INDICATE A PATTERN OF MISSINGNESS, SINCE ROWS WITH MISSING PRICES ALSO HAVE OTHER MISSING ATTRIBUTES.\n",
        "# WE COULD CHECK IF THERE IS MISSINGNESS ACROSS, FOR EXAMPLE, A CERTAIN TYPE OF LISTING, THEN THE RESULTS WILL BE SKEWED."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3HKQpHT7-Tc",
        "outputId": "50952e92-9520-488c-fdab-cdc578adf9d1"
      },
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drop_missing_prices results:\n",
            " accommodates    0\n",
            "bedrooms        1\n",
            "beds            1\n",
            "dtype: int64 \n",
            "\n",
            "impute_price results:\n",
            " accommodates       0\n",
            "bedrooms         158\n",
            "beds            2240\n",
            "dtype: int64 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WE ARE DOING THE FOLLOWING ONLY ON THE PRICE IMPUTATION DF, SINCE THE DROP PRICE DF HAS ONLY 1 NULL VALUE.\n",
        "# Only use rows where all three variables are valid (non-missing, non-zero)\n",
        "valid = df_price_imputation[\n",
        "    (df_price_imputation['accommodates'] > 0) &\n",
        "    (df_price_imputation['beds'] > 0) &\n",
        "    (df_price_imputation['bedrooms'] > 0)\n",
        "]\n",
        "\n",
        "# Compute ratios\n",
        "valid['guests_per_bed'] = valid['accommodates'] / valid['beds']\n",
        "valid['beds_per_bedroom'] = valid['beds'] / valid['bedrooms']\n",
        "valid['guests_per_bedroom'] = valid['accommodates'] / valid['bedrooms']\n",
        "\n",
        "# Get averages and medians\n",
        "summary = valid[['guests_per_bed', 'beds_per_bedroom', 'guests_per_bedroom']].agg(['mean', 'median'])\n",
        "print(\"Statistics before imputation:\\n\", summary)\n",
        "\n",
        "# Use medians for integers\n",
        "guests_per_bed = summary.loc['median', 'guests_per_bed']\n",
        "beds_per_bedroom = summary.loc['median', 'beds_per_bedroom']\n",
        "guests_per_bedroom = summary.loc['median', 'guests_per_bedroom']\n",
        "\n",
        "# Impute median values where missing\n",
        "# For missing beds but nonmissing bedrooms, impute median beds per bedroom\n",
        "df_price_imputation.loc[df_price_imputation['beds'].isna() & df_price_imputation['bedrooms'].notna(), 'beds'] = df_price_imputation['bedrooms'] * beds_per_bedroom\n",
        "\n",
        "# For remaining missing beds, divide max guest count by median guests per bed\n",
        "df_price_imputation.loc[df_price_imputation['beds'].isna(), 'beds'] = df_price_imputation['accommodates'] / guests_per_bed\n",
        "\n",
        "# For missing bedrooms but nonmissing bedrooms, divide beds by beds per bedroom\n",
        "df_price_imputation.loc[df_price_imputation['bedrooms'].isna() & df_price_imputation['beds'].notna(), 'bedrooms'] = df_price_imputation['beds'] / beds_per_bedroom\n",
        "\n",
        "# For remaining missing bedrooms, divide max guest count by median guests per bedroom\n",
        "df_price_imputation.loc[df_price_imputation['bedrooms'].isna(), 'bedrooms'] = df_price_imputation['accommodates'] / guests_per_bedroom\n",
        "\n",
        "# Re-check missingness\n",
        "print(\"\\nRE-CHECKING MISSINGNESS ON PRICE IMPUTATION DATAFRAME:\\n\", df_price_imputation[['accommodates','bedrooms','beds']].isna().sum())\n",
        "\n",
        "# DROP THE 1 MISSING VALUE FOR THE DROP PRICE DATAFRAME\n",
        "df_drop_price = df_drop_price.dropna(subset=[\"beds\"])\n",
        "df_drop_price = df_drop_price.dropna(subset=[\"bedrooms\"])\n",
        "\n",
        "# Show the results\n",
        "print(\"\\nRE-CHECKING MISSINGNESS ON DROP PRICE DATAFRAME:\\n\", df_drop_price[['accommodates','bedrooms','beds']].isna().sum())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UxeNXHZ8COJ",
        "outputId": "ce57e0ae-25e0-47ee-eb15-d1e8e0de12cb"
      },
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistics before imputation:\n",
            "         guests_per_bed  beds_per_bedroom  guests_per_bedroom\n",
            "mean          1.983956          1.225566            2.261351\n",
            "median        2.000000          1.000000            2.000000\n",
            "\n",
            "RE-CHECKING MISSINGNESS ON PRICE IMPUTATION DATAFRAME:\n",
            " accommodates    0\n",
            "bedrooms        0\n",
            "beds            0\n",
            "dtype: int64\n",
            "\n",
            "RE-CHECKING MISSINGNESS ON DROP PRICE DATAFRAME:\n",
            " accommodates    0\n",
            "bedrooms        0\n",
            "beds            0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLEAN UP MISSING \"bathroom\" AND \"bathroom_text\" ATTRIBUTES"
      ],
      "metadata": {
        "id": "Ftrlg55IYzFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some properties are missing a number of bathrooms in the \"bathrooms\" attribute. Instead, it's stored as a string in the \"bathrooms_text\" attribute.\n",
        "\n",
        "# AGAIN, THIS RESULT COULD INDICATE A PATTERN OF MISSINGNESS, SINCE ROWS WITH MISSING PRICES ALSO HAVE OTHER MISSING ATTRIBUTES.\n",
        "# WE COULD CHECK IF THERE IS MISSINGNESS ACROSS, FOR EXAMPLE, A CERTAIN TYPE OF LISTING, THEN THE RESULTS WILL BE SKEWED.\n",
        "\n",
        "for name, df in datasets.items():\n",
        "  print(f\"CHECKING NULL COUNTS ON {name} DATAFRAME:\\n\", df[[\"bathrooms\", \"bathrooms_text\"]].isna().sum().to_frame(name=\"Null Count\"))\n",
        "  print(f\"\\nCHECKING MISSINGNESS ON {name} DATAFRAME:\\n\", df.loc[df[\"bathrooms\"].isna(),[\"bathrooms\",\"bathrooms_text\"]].head(10),\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQGCWqkpYyoM",
        "outputId": "08ece696-f1db-46d8-cc92-184af4c89a70"
      },
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHECKING NULL COUNTS ON drop_missing_prices DATAFRAME:\n",
            "                 Null Count\n",
            "bathrooms                2\n",
            "bathrooms_text           2\n",
            "\n",
            "CHECKING MISSINGNESS ON drop_missing_prices DATAFRAME:\n",
            "        bathrooms bathrooms_text\n",
            "106          NaN            NaN\n",
            "19459        NaN            NaN \n",
            "\n",
            "CHECKING NULL COUNTS ON impute_price DATAFRAME:\n",
            "                 Null Count\n",
            "bathrooms             2241\n",
            "bathrooms_text           2\n",
            "\n",
            "CHECKING MISSINGNESS ON impute_price DATAFRAME:\n",
            "      bathrooms bathrooms_text\n",
            "7          NaN      1.5 baths\n",
            "8          NaN  1 shared bath\n",
            "9          NaN      1.5 baths\n",
            "38         NaN         1 bath\n",
            "44         NaN         1 bath\n",
            "54         NaN         1 bath\n",
            "61         NaN         1 bath\n",
            "83         NaN         1 bath\n",
            "98         NaN         1 bath\n",
            "106        NaN            NaN \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WE ARE DOING THE FOLLOWING ONLY ON THE PRICE IMPUTATION DF, SINCE THE DROP PRICE DF HAS ONLY 2 NULL VALUES.\n",
        "# Extract the numeric part from bathrooms_text\n",
        "bathrooms_from_text = (\n",
        "    df_price_imputation[\"bathrooms_text\"]\n",
        "    .astype(str)\n",
        "    .str.extract(r'(\\d+(\\.\\d+)?)')[0]   # capture integers or decimals\n",
        "    .astype(float)\n",
        ")\n",
        "\n",
        "# Fill missing values in bathrooms with extracted numbers\n",
        "df_price_imputation[\"bathrooms\"] = df_price_imputation[\"bathrooms\"].fillna(bathrooms_from_text)"
      ],
      "metadata": {
        "id": "HDANKDmEZ20f"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_price_imputation[\"bathrooms\"].isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsTRHD5-e2Op",
        "outputId": "61a57249-9fe2-4519-ed88-348311c288ce"
      },
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(21)"
            ]
          },
          "metadata": {},
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WE ARE DOING THE FOLLOWING ONLY ON THE PRICE IMPUTATION DF, SINCE THE DROP PRICE DF HAS ONLY 2 NULL VALUES.\n",
        "# Drop rest of missing values since the row count is so low\n",
        "\n",
        "df_price_imputation = df_price_imputation.dropna(subset=['bathrooms'])"
      ],
      "metadata": {
        "id": "n0RT_lzuewgv"
      },
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_price_imputation[\"bathrooms\"].isna().sum()"
      ],
      "metadata": {
        "id": "Xo4Jm6TifyQH",
        "outputId": "f65c92e6-31e7-401e-a61e-02367c1b315d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(0)"
            ]
          },
          "metadata": {},
          "execution_count": 246
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the pre results\n",
        "print(\"\\nCHECKING NULL COUNTS ON PRICE DROP DATAFRAME BEFORE:\\n\", df_drop_price[[\"bathrooms\", \"bathrooms_text\"]].isna().sum().to_frame(name=\"Null Count\"))\n",
        "\n",
        "# DROP THE 2 MISSING VALUES FOR THE DROP PRICE DATAFRAME\n",
        "df_drop_price = df_drop_price.dropna(subset=[\"bathrooms\"])\n",
        "df_drop_price = df_drop_price.dropna(subset=[\"bathrooms_text\"])\n",
        "\n",
        "# Show the post results\n",
        "print(\"\\nCHECKING NULL COUNTS ON PRICE DROP DATAFRAME AFTER:\\n\", df_drop_price[[\"bathrooms\", \"bathrooms_text\"]].isna().sum().to_frame(name=\"Null Count\"))"
      ],
      "metadata": {
        "id": "aH4lfkR7f07o",
        "outputId": "631f9d88-fd66-4034-fd93-a3f5ac7776a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CHECKING NULL COUNTS ON PRICE DROP DATAFRAME BEFORE:\n",
            "                 Null Count\n",
            "bathrooms                2\n",
            "bathrooms_text           2\n",
            "\n",
            "CHECKING NULL COUNTS ON PRICE DROP DATAFRAME AFTER:\n",
            "                 Null Count\n",
            "bathrooms                0\n",
            "bathrooms_text           0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRANSFORM \"description\" ATTRIBUTE BY ADDING \"description_missing\" AND \"description_length\" ATTRIBUTES"
      ],
      "metadata": {
        "id": "SOFawNzn8R0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Description has some missing values\n",
        "for name, df in datasets.items():\n",
        "  print(f\"CHECK MISSING DESCRIPTION VALUE COUNT FOR {name} DATAFRAME:\\n\", df[\"description\"].isna().sum(),\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkSO-oBS8Top",
        "outputId": "d9f1ab84-5e88-4161-d5cc-1c4a4b641baf"
      },
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHECK MISSING DESCRIPTION VALUE COUNT FOR drop_missing_prices DATAFRAME:\n",
            " 134 \n",
            "\n",
            "CHECK MISSING DESCRIPTION VALUE COUNT FOR impute_price DATAFRAME:\n",
            " 176 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, df in datasets.items():\n",
        "  df['description_missing'] = df['description'].isna().astype(int) # Make flag for missing description\n",
        "  df['description_length'] = df['description'].fillna('').str.len() # Add description length\n",
        "\n",
        "# Show the results of adding the above columns\n",
        "for name, df in datasets.items():\n",
        "  print(f\"\\ndescription_missing HEAD FOR {name} DATAFRAME:\\n\", df['description_missing'].head(10))\n",
        "  print(f\"\\ndescription_length HEAD FOR {name} DATAFRAME:\\n\", df['description_length'].head(10))\n",
        "\n",
        "# We are now dropping the \"description\" attribute, as we no longer need it (we are not doing any text analysis).\n",
        "for name, df in datasets.items():\n",
        "  datasets[name] = df.drop(columns=[\"description\"])"
      ],
      "metadata": {
        "id": "e9_SK2cw8X0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24bf061d-f14f-43a8-80bc-3bb82a6d16e4"
      },
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "description_missing HEAD FOR drop_missing_prices DATAFRAME:\n",
            " 2     0\n",
            "4     0\n",
            "5     0\n",
            "10    0\n",
            "14    0\n",
            "16    0\n",
            "17    0\n",
            "18    0\n",
            "21    1\n",
            "22    1\n",
            "Name: description_missing, dtype: int64\n",
            "\n",
            "description_length HEAD FOR drop_missing_prices DATAFRAME:\n",
            " 2     482\n",
            "4     412\n",
            "5      75\n",
            "10     54\n",
            "14    170\n",
            "16    290\n",
            "17    230\n",
            "18    352\n",
            "21      0\n",
            "22      0\n",
            "Name: description_length, dtype: int64\n",
            "\n",
            "description_missing HEAD FOR impute_price DATAFRAME:\n",
            " 2     0\n",
            "4     0\n",
            "5     0\n",
            "7     0\n",
            "8     0\n",
            "9     0\n",
            "10    0\n",
            "14    0\n",
            "16    0\n",
            "17    0\n",
            "Name: description_missing, dtype: int64\n",
            "\n",
            "description_length HEAD FOR impute_price DATAFRAME:\n",
            " 2     482\n",
            "4     412\n",
            "5      75\n",
            "7     558\n",
            "8     515\n",
            "9     471\n",
            "10     54\n",
            "14    170\n",
            "16    290\n",
            "17    230\n",
            "Name: description_length, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRANSFORM \"host_since\" ATTRIBUTE BY ADDING \"host_tenure_days\" ATTRIBUTE"
      ],
      "metadata": {
        "id": "O4Vocm0Z8h2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data has a host_since feature, so we use it to create a \"host_tenure_days\" column for each dataframe.\n",
        "\n",
        "for name, df in datasets.items():\n",
        "  df['host_since'] = pd.to_datetime(df['host_since'], errors='coerce')\n",
        "  latest_scrape = pd.to_datetime(df['last_scraped']).max() # Isolate the latest scrape date for calculation\n",
        "  df['host_tenure_days'] = (latest_scrape - df['host_since']).dt.days # Create new \"host_tenure_days\" attribute\n",
        "\n",
        "# Drop \"host_since\" and \"last_scraped\" columns\n",
        "for name, df in datasets.items():\n",
        "  datasets[name] = df.drop(columns=[\"host_since\", \"last_scraped\"])"
      ],
      "metadata": {
        "id": "RO9apSRT8kBd"
      },
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, df in datasets.items():\n",
        "  print(f\"{name} results: \",df[\"host_tenure_days\"].isna().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbnQT1Fb8uue",
        "outputId": "b077de09-7801-4dab-cd27-8b9686778d92"
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drop_missing_prices results:  0\n",
            "impute_price results:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRANSFORM \"host_about\" ATTRIBUTE BY ADDING \"host_about_missing\" AND \"host_about_length\" ATTRIBUTES"
      ],
      "metadata": {
        "id": "nZN6lPWn83fX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, df in datasets.items():\n",
        "  df['host_about_missing'] = df['host_about'].isna().astype(int) # Make flag for host about\n",
        "  df['host_about_length'] = df['host_about'].fillna('').str.len() # Host about length\n",
        "\n",
        "# We are now dropping the \"host_about\" attribute, as we are not doing any text analysis\n",
        "for name, df in datasets.items():\n",
        "  datasets[name] = df.drop(columns=[\"host_about\"])"
      ],
      "metadata": {
        "id": "JzlfaWDS88tr"
      },
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRANSFORM \"host_response_rate\" AND \"host_acceptance_rate\" BY ADDING \"...missing\" ATTRIBUTES"
      ],
      "metadata": {
        "id": "ty9yKcxo9Koo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Format as float, add missing flag, fill with median\n",
        "\n",
        "for name, df in datasets.items():\n",
        "  for col in [\"host_response_rate\", \"host_acceptance_rate\"]:\n",
        "      temp = df[col].astype(str).str.strip().str.rstrip('%').replace('', np.nan)\n",
        "      df[col] = pd.to_numeric(temp, errors='coerce')\n",
        "      df[f\"{col}_missing\"] = df[col].isna().astype(int)\n",
        "      df[col] = df[col].fillna(df[col].median())"
      ],
      "metadata": {
        "id": "s8RKLNx59NKV"
      },
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRANSFORM \"host_response_time\" ATTRIBUTE BY ADDING CATEGORY FOR MISSING VALUES CALLED \"unknown\""
      ],
      "metadata": {
        "id": "DFPeo21O9zTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Response time is a category\n",
        "# Fill unknown or missing response time with category \"unknown\"\n",
        "\n",
        "for name, df in datasets.items():\n",
        "  df['host_response_time'] = df['host_response_time'].fillna('unknown')"
      ],
      "metadata": {
        "id": "cf3LCsRc91De"
      },
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRANSFORM \"neighborhood_overview\" ATTRIBUTE BY ADDING \"neighborhood_overview_length\" AND \"neighborhood_overview_missing\" ATTRIBUTES"
      ],
      "metadata": {
        "id": "zN7sj4IcEmZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, df in datasets.items():\n",
        "  df['neighborhood_overview_missing'] = df['neighborhood_overview'].isna().astype(int) # Make flag for neighboorhood overview\n",
        "  df['neighborhood_overview_length'] = df['neighborhood_overview'].fillna('').str.len() # Neighborhood overview length\n",
        "\n",
        "# We are now dropping the \"neighborhood_overview\" attribute, as we are not doing any text analysis\n",
        "for name, df in datasets.items():\n",
        "  df.drop(columns=[\"neighborhood_overview\"], inplace=True)\n",
        "\n",
        "# Print results\n",
        "for name, df in datasets.items():\n",
        "  print(f\"{name} results: \",df[[\"neighborhood_overview_missing\",\"neighborhood_overview_length\"]].isna().sum())"
      ],
      "metadata": {
        "id": "fMrZaqQHEmy5",
        "outputId": "e1da5adb-2719-428d-b0e0-b98887ca843d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drop_missing_prices results:  neighborhood_overview_missing    0\n",
            "neighborhood_overview_length     0\n",
            "dtype: int64\n",
            "impute_price results:  neighborhood_overview_missing    0\n",
            "neighborhood_overview_length     0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(df_drop_price)"
      ],
      "metadata": {
        "id": "XXNs9rVdd5_6",
        "outputId": "f08a040b-637c-4577-f48c-757db3da14da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['last_scraped',\n",
              " 'description',\n",
              " 'neighborhood_overview',\n",
              " 'host_since',\n",
              " 'host_location',\n",
              " 'host_about',\n",
              " 'host_response_time',\n",
              " 'host_response_rate',\n",
              " 'host_acceptance_rate',\n",
              " 'host_is_superhost',\n",
              " 'host_has_profile_pic',\n",
              " 'host_identity_verified',\n",
              " 'neighbourhood_cleansed',\n",
              " 'latitude',\n",
              " 'longitude',\n",
              " 'property_type',\n",
              " 'room_type',\n",
              " 'accommodates',\n",
              " 'bathrooms',\n",
              " 'bathrooms_text',\n",
              " 'bedrooms',\n",
              " 'beds',\n",
              " 'amenities',\n",
              " 'price',\n",
              " 'minimum_nights',\n",
              " 'maximum_nights',\n",
              " 'availability_30',\n",
              " 'number_of_reviews',\n",
              " 'instant_bookable',\n",
              " 'calculated_host_listings_count']"
            ]
          },
          "metadata": {},
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRANSFORM \"host_location\" ATTRIBUTE BY ADDING \"host_location_missing\" ATTRIBUTE"
      ],
      "metadata": {
        "id": "vSz6N5qjHEV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, df in datasets.items():\n",
        "  df['host_location_missing'] = df['host_location'].isna().astype(int) # Make flag for host location\n",
        "\n",
        "# We are now dropping the \"host_location\" attribute, as we are not doing any text analysis\n",
        "for name, df in datasets.items():\n",
        "  df.drop(columns=[\"host_location\"], inplace=True)\n",
        "\n",
        "# Print results\n",
        "for name, df in datasets.items():\n",
        "  print(f\"{name} results: \",df[[\"host_location_missing\"]].isna().sum())"
      ],
      "metadata": {
        "id": "snoFSDaoHNEP",
        "outputId": "47cabe77-85e3-43eb-8991-eca279597f47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'host_location'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'host_location'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-992645623.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'host_location_missing'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'host_location'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Make flag for host location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# We are now dropping the \"host_location\" attribute, as we are not doing any text analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'host_location'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_drop_price.sort_values(by=\"price\", ascending=True).head(10)\n",
        "df_drop_price.shape"
      ],
      "metadata": {
        "id": "68TBxyO-JG8b",
        "outputId": "244cede8-54e5-4dba-c71d-12e4ea43bab7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7632, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [TEMP] DROP COLUMNS THAT WE WILL COME BACK TO LATER"
      ],
      "metadata": {
        "id": "XMpcxAXGZxpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(df_drop_price)"
      ],
      "metadata": {
        "id": "7gg_dUoaZ3Dg",
        "outputId": "aeaf7314-a31a-498c-c2c0-e00d9ca3a0af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['last_scraped',\n",
              " 'description',\n",
              " 'neighborhood_overview',\n",
              " 'host_since',\n",
              " 'host_location',\n",
              " 'host_about',\n",
              " 'host_response_time',\n",
              " 'host_response_rate',\n",
              " 'host_acceptance_rate',\n",
              " 'host_is_superhost',\n",
              " 'host_has_profile_pic',\n",
              " 'host_identity_verified',\n",
              " 'neighbourhood_cleansed',\n",
              " 'latitude',\n",
              " 'longitude',\n",
              " 'property_type',\n",
              " 'room_type',\n",
              " 'accommodates',\n",
              " 'bathrooms',\n",
              " 'bathrooms_text',\n",
              " 'bedrooms',\n",
              " 'beds',\n",
              " 'amenities',\n",
              " 'price',\n",
              " 'minimum_nights',\n",
              " 'maximum_nights',\n",
              " 'availability_30',\n",
              " 'number_of_reviews',\n",
              " 'instant_bookable',\n",
              " 'calculated_host_listings_count']"
            ]
          },
          "metadata": {},
          "execution_count": 225
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CREATE HEX BINS FROM LONG/LAT TO ADD A MORE GRANULAR LOCATION ATTRIBUTE"
      ],
      "metadata": {
        "id": "3otU4nvDBx6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Neighborhood data is not very granular\n",
        "# Long and lat used to create hexbins\n",
        "\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point, Polygon\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- 1. Create GeoDataFrame from lat / lon ---\n",
        "\n",
        "gdf = gpd.GeoDataFrame(\n",
        "    df.copy(),\n",
        "    geometry=gpd.points_from_xy(df[\"longitude\"], df[\"latitude\"]),\n",
        "    crs=\"EPSG:4326\"   # WGS84 (lat/lon)\n",
        ")\n",
        "\n",
        "# Project to a metric CRS (UTM zone  here: 32N, good for Denmark/southern Sweden)\n",
        "gdf = gdf.to_crs(epsg=32632)\n",
        "\n",
        "\n",
        "# --- 2. Helper: build a single regular hexagon around a center ---\n",
        "\n",
        "def make_hexagon(cx, cy, radius):\n",
        "    \"\"\"\n",
        "    Create a pointy-top regular hexagon centered at (cx, cy)\n",
        "    with given radius (distance from center to each vertex).\n",
        "    \"\"\"\n",
        "    # Pointy-top: start at 30 and step by 60\n",
        "    angles = np.deg2rad(np.arange(0, 360, 60) + 30)\n",
        "    coords = [(cx + radius * np.cos(a), cy + radius * np.sin(a)) for a in angles]\n",
        "    return Polygon(coords)\n",
        "\n",
        "\n",
        "# --- 3. Build a hex grid over the extent of gdf ---\n",
        "\n",
        "def make_hex_grid(gdf, radius):\n",
        "    \"\"\"\n",
        "    Create a pointy-top hexagon grid covering the extent of gdf.\n",
        "    radius = distance from hex center to each vertex (in CRS units, e.g. meters).\n",
        "    \"\"\"\n",
        "    xmin, ymin, xmax, ymax = gdf.total_bounds\n",
        "\n",
        "    # Pointy-top spacing (Red Blob Games):\n",
        "    # horizontal distance between centers = sqrt(3) * radius\n",
        "    # vertical distance between rows = 1.5 * radius\n",
        "    dx = np.sqrt(3) * radius\n",
        "    dy = 1.5 * radius\n",
        "\n",
        "    cols = np.arange(xmin - dx, xmax + dx, dx)\n",
        "    rows = np.arange(ymin - dy, ymax + dy, dy)\n",
        "\n",
        "    hexes = []\n",
        "    for row_idx, cy in enumerate(rows):\n",
        "        for col_idx, cx in enumerate(cols):\n",
        "            # Offset every second row by half the horizontal spacing\n",
        "            cx_shifted = cx + (dx / 2.0 if row_idx % 2 == 1 else 0.0)\n",
        "            hex_poly = make_hexagon(cx_shifted, cy, radius)\n",
        "            hexes.append(hex_poly)\n",
        "\n",
        "    hex_grid = gpd.GeoDataFrame(\n",
        "        {\"hex_id\": range(len(hexes))},\n",
        "        geometry=hexes,\n",
        "        crs=gdf.crs\n",
        "    )\n",
        "    return hex_grid\n",
        "\n",
        "\n",
        "# --- 4. Generate hex grid + optional trimming ---\n",
        "\n",
        "hex_radius = 250  # meters\n",
        "hex_grid = make_hex_grid(gdf, hex_radius)\n",
        "\n",
        "# Optional trimming to a buffered convex hull of listings\n",
        "study_area = gdf.geometry.union_all().convex_hull.buffer(2 * hex_radius)\n",
        "hex_grid = hex_grid[hex_grid.intersects(study_area)].reset_index(drop=True)\n",
        "hex_grid[\"hex_id\"] = hex_grid.index  # reindex after trimming\n",
        "\n",
        "# --- 5. Spatial join: assign each listing to a hex ---\n",
        "\n",
        "joined = gpd.sjoin(\n",
        "    gdf,\n",
        "    hex_grid[[\"hex_id\", \"geometry\"]],\n",
        "    how=\"left\",\n",
        "    predicate=\"within\"   # use \"intersects\" if you see edge-cases\n",
        ")\n",
        "\n",
        "# Join hex_id to the original df\n",
        "\n",
        "merged = df.merge(joined[[\"id\", \"hex_id\"]], on=\"id\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "k6i9qRlWOb8X",
        "outputId": "a35a9d56-cff1-4c5c-db28-199f287d053e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['id'] not in index\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1730642063.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m# Join hex_id to the original df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mmerged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoined\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hex_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/geopandas/geodataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1894\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mGeoDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1895\u001b[0m         \"\"\"\n\u001b[0;32m-> 1896\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1897\u001b[0m         \u001b[0;31m# Custom logic to avoid waiting for pandas GH51895\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1898\u001b[0m         \u001b[0;31m# result is not geometry dtype for multi-indexes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6254\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['id'] not in index\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Visualization: check that hexes look like hexes ---\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "\n",
        "# plot hex outlines\n",
        "hex_grid.boundary.plot(ax=ax, linewidth=0.5)\n",
        "\n",
        "# plot listing points\n",
        "gdf.plot(ax=ax, markersize=3, color=\"red\", alpha=0.7)\n",
        "\n",
        "ax.set_title(\"Listings and true hex grid\")\n",
        "ax.set_axis_off()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2wDJeMUii8yD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mapping hexes"
      ],
      "metadata": {
        "id": "_cXkDpNzOE5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "\n",
        "# Convert hexes and points to WGS84 (lat/lon)\n",
        "hex_wgs = hex_grid.to_crs(epsg=4326)\n",
        "pts_wgs = gdf.to_crs(epsg=4326)\n",
        "\n",
        "center_lat = pts_wgs.geometry.y.mean()\n",
        "center_lon = pts_wgs.geometry.x.mean()\n",
        "\n",
        "m = folium.Map(location=[center_lat, center_lon], zoom_start=11)\n",
        "\n",
        "folium.GeoJson(\n",
        "    hex_wgs,\n",
        "    name=\"Hex grid\",\n",
        "    style_function=lambda feature: {\n",
        "        \"fillColor\": \"none\",\n",
        "        \"color\": \"blue\",\n",
        "        \"weight\": 1,\n",
        "        \"fillOpacity\": 0.1,\n",
        "    },\n",
        ").add_to(m)\n",
        "\n",
        "sample_pts = pts_wgs.sample(min(2000, len(pts_wgs)), random_state=0)\n",
        "\n",
        "for _, row in sample_pts.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[row.geometry.y, row.geometry.x],\n",
        "        radius=2,\n",
        "        color=\"red\",\n",
        "        fill=True,\n",
        "        fill_opacity=0.7,\n",
        "    ).add_to(m)\n",
        "\n",
        "m"
      ],
      "metadata": {
        "id": "PPo6hfQyHSNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PLACEHOLDER FOR MODELING"
      ],
      "metadata": {
        "id": "zsnj8tLGfToY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------\n",
        "# 0. Wrap your two datasets in a dictionary - NOTE TO GROUP: VALIDATE THAT THE 2 DATASETS WORK, OR PROCEED WITH JUST 1 AT A TIME.\n",
        "# --------------------------------------------------\n",
        "datasets = {\n",
        "    \"price_imputation\": df_price_imputation,\n",
        "    \"drop_price\": df_drop_price,\n",
        "}\n",
        "\n",
        "# Replace this with the actual column name in your data\n",
        "target_col = \"occupancy_30\"   # e.g. number of booked days in next 30\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 1. Define candidate models (supervised regressors) - NOTE TO GROUP: ANY OTHER MODELS TO CONSIDER?\n",
        "# --------------------------------------------------\n",
        "models = {\n",
        "    \"LinearRegression\": LinearRegression(),\n",
        "    \"Ridge\": Ridge(),\n",
        "    \"Lasso\": Lasso(),\n",
        "    \"KNN\": KNeighborsRegressor(),\n",
        "    \"DecisionTree\": DecisionTreeRegressor(),\n",
        "    \"RandomForest\": RandomForestRegressor(),\n",
        "    \"GradientBoosting\": GradientBoostingRegressor(),\n",
        "}\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 2. Define hyperparameter grids for each model - NOTE TO GROUP: GET THE PIPELINE TO CONSIDER ALL HYPERPARAMETERS INSTEAD OF JUST ARBITRARY NUMBERS\n",
        "# --------------------------------------------------\n",
        "param_grids = {\n",
        "    \"LinearRegression\": {},  # no hyperparameters\n",
        "\n",
        "    \"Ridge\": {\n",
        "        \"model__alpha\": [0.1, 1.0, 10.0],\n",
        "    },\n",
        "\n",
        "    \"Lasso\": {\n",
        "        \"model__alpha\": [0.001, 0.01, 0.1],\n",
        "    },\n",
        "\n",
        "    \"KNN\": {\n",
        "        \"model__n_neighbors\": [3, 5, 7, 15],\n",
        "        \"model__weights\": [\"uniform\", \"distance\"],\n",
        "    },\n",
        "\n",
        "    \"DecisionTree\": {\n",
        "        \"model__max_depth\": [3, 5, 10, None],\n",
        "        \"model__min_samples_split\": [2, 10, 30],\n",
        "    },\n",
        "\n",
        "    \"RandomForest\": {\n",
        "        \"model__n_estimators\": [100, 300],\n",
        "        \"model__max_depth\": [5, 10, None],\n",
        "    },\n",
        "\n",
        "    \"GradientBoosting\": {\n",
        "        \"model__learning_rate\": [0.01, 0.1],\n",
        "        \"model__n_estimators\": [100, 200],\n",
        "    },\n",
        "}\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 3. Run the full pipeline for each dataset -\n",
        "# --------------------------------------------------\n",
        "all_results = []\n",
        "\n",
        "for ds_name, df in datasets.items():\n",
        "    print(f\"\\n================ {ds_name} ================\\n\")\n",
        "\n",
        "    # --- 3.1 Split into features (X) and target (y)\n",
        "    X = df.drop(columns=[target_col])\n",
        "    y = df[target_col]\n",
        "\n",
        "    # --- 3.2 Train/test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # --- 3.3 Identify numeric vs categorical columns\n",
        "    numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    categorical_cols = X_train.select_dtypes(\n",
        "        include=[\"object\", \"category\"]\n",
        "    ).columns.tolist()\n",
        "\n",
        "    # --- 3.4 Preprocessing: scaling + one-hot encoding\n",
        "    preprocess = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", StandardScaler(), numeric_cols),\n",
        "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # --- 3.5 Loop over models and run GridSearchCV\n",
        "    for model_name, model in models.items():\n",
        "        print(f\"Fitting model: {model_name} on {ds_name}...\")\n",
        "\n",
        "        pipe = Pipeline(steps=[\n",
        "            (\"preprocess\", preprocess),\n",
        "            (\"model\", model),\n",
        "        ])\n",
        "\n",
        "        grid = GridSearchCV(\n",
        "            estimator=pipe,\n",
        "            param_grid=param_grids[model_name],\n",
        "            cv=5,\n",
        "            scoring=\"neg_mean_absolute_error\",\n",
        "            n_jobs=-1,\n",
        "        )\n",
        "\n",
        "        grid.fit(X_train, y_train)\n",
        "\n",
        "        best_model = grid.best_estimator_\n",
        "\n",
        "        # --- 3.6 Evaluate on test set\n",
        "        y_pred = best_model.predict(X_test)\n",
        "\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "\n",
        "        # Guard against division by zero in MAPE\n",
        "        non_zero_mask = y_test != 0\n",
        "        if non_zero_mask.sum() > 0:\n",
        "            mape = np.mean(\n",
        "                np.abs((y_test[non_zero_mask] - y_pred[non_zero_mask]) / y_test[non_zero_mask])\n",
        "            ) * 100\n",
        "        else:\n",
        "            mape = np.nan\n",
        "\n",
        "        all_results.append({\n",
        "            \"Dataset\": ds_name,\n",
        "            \"Model\": model_name,\n",
        "            \"Best Params\": grid.best_params_,\n",
        "            \"MAE\": mae,\n",
        "            \"RMSE\": rmse,\n",
        "            \"MAPE\": mape,\n",
        "        })\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 4. Collect and inspect results\n",
        "# --------------------------------------------------\n",
        "results_df = pd.DataFrame(all_results)\n",
        "results_df = results_df.sort_values(by=[\"Dataset\", \"MAE\"])\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "fSkIdf3ZfXk9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}